{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM1Bjh9JK3V3",
        "outputId": "ec9b6ad2-ab1f-49e3-a175-f862e0ea118d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install janome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlxMOSNfczqQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td2bIj3uc24b"
      },
      "outputs": [],
      "source": [
        "INPUT_DIR = \"å¤ç›®æ¼±çŸ³ä½œå“é›†\"\n",
        "os.makedirs(INPUT_DIR,exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8y9ICrCeNjZ",
        "outputId": "aee87ffa-1ebf-4639-e203-033c010c66c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "åˆè¨ˆ 22 å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "raw_text = \"\"\n",
        "file_paths = glob.glob(os.path.join(INPUT_DIR, '*.txt'))\n",
        "total_files = len(file_paths)\n",
        "\n",
        "if total_files == 0:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: '{INPUT_DIR}' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«.txtãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "else:\n",
        "    print(f\"åˆè¨ˆ {total_files} å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JelOqMHwuC8p"
      },
      "source": [
        "# textã®çµåˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWLCype4eotw",
        "outputId": "4bccad7b-a2ae-4c97-ce8e-348356257e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "å…¨ä½œå“ã®çµåˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·æ–‡å­—æ•°: 3117434\n"
          ]
        }
      ],
      "source": [
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€ã¤ãšã¤èª­ã¿è¾¼ã¿ã€å†…å®¹ã‚’çµåˆ\n",
        "    for file_path in file_paths:\n",
        "        file_name = os.path.basename(file_path)\n",
        "        try:\n",
        "            # é’ç©ºæ–‡åº«ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯Shift_JISã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§é–‹ã\n",
        "            with open(file_path, 'r', encoding='shift_jis') as f:\n",
        "                content = f.read()\n",
        "                raw_text += content\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"è­¦å‘Š: ãƒ•ã‚¡ã‚¤ãƒ« '{file_name}' ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
        "        except FileNotFoundError:\n",
        "            # globã§è¦‹ã¤ã‹ã£ã¦ã„ã‚‹ã¯ãšãªã®ã§é€šå¸¸ã¯ç™ºç”Ÿã—ãªã„\n",
        "            continue\n",
        "\n",
        "    print(f\"å…¨ä½œå“ã®çµåˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚ç·æ–‡å­—æ•°: {len(raw_text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQFHOXyPgR5s"
      },
      "outputs": [],
      "source": [
        "cleaned_text = re.sub(r'ã€Š.*?ã€‹','',raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "V4ZE5jrZhy2F",
        "outputId": "2add9515-82fa-4e3a-c8cf-eeb3ee6c6dd5"
      },
      "outputs": [],
      "source": [
        "cleaned_text = cleaned_text.replace(\"|\",\"\")\n",
        "cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5eyc_essiVA1",
        "outputId": "e6a3df2a-8308-4b68-ebe9-f5147ed42197"
      },
      "outputs": [],
      "source": [
        "    cleaned_text = re.sub(r'ï¼»ï¼ƒ.*?ï¼½', '', cleaned_text, flags=re.DOTALL)\n",
        "    cleaned_text = re.sub(r'ï¼».*?ï¼½', '', cleaned_text) # å¿µã®ãŸã‚æ®‹ã‚Šã®æ³¨é‡ˆã‚‚é™¤å»\n",
        "    cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Y7eA64Dix_z"
      },
      "outputs": [],
      "source": [
        "cleaned_text = re.sub(r'\\n-{10,}\\n.*?\\n-{10,}\\n', '\\n', cleaned_text, flags=re.DOTALL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKAiACJGjqY1"
      },
      "outputs": [],
      "source": [
        "cleaned_text = re.sub(r'[\\r\\n\\t\\sã€€]+', ' ', cleaned_text).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "wo3jCOWTnUDL",
        "outputId": "032ff49a-414d-4327-b9c9-4c321acb1c5b"
      },
      "outputs": [],
      "source": [
        "cleaned_text = cleaned_text.replace('ï½œ','').replace('â€»','')\n",
        "cleaned_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icVmW9TbofN8"
      },
      "outputs": [],
      "source": [
        "punctuations = 'ã€Œã€ã€ã€ï¼ˆï¼‰ã€ã€‘ã€ã€‚ï¼ï¼Ÿãƒ»â€¦ãƒ¼'\n",
        "for p in punctuations:\n",
        "    cleaned_text = cleaned_text.replace(p, '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "DBZMi59dpAlP",
        "outputId": "3ff9fca5-f528-42fb-ee1e-24d71f475653"
      },
      "outputs": [],
      "source": [
        "cleaned_text = cleaned_text.replace('å¤ç›®æ¼±çŸ³','')\n",
        "cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "NIdkyJVEq02b",
        "outputId": "9e9f9142-eb51-4d20-a507-4f96c013293e"
      },
      "outputs": [],
      "source": [
        "cleaned_text = re.sub(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]|[\\d])[\\sã€€]', '', cleaned_text)\n",
        "cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1DjMq05rAmI",
        "outputId": "b22cd3a9-db22-40f1-e636-8bad32b01650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ 'soseki_corpus_preprocessed.txt' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_FILE = 'soseki_corpus_preprocessed.txt'\n",
        "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "    f.write(cleaned_text)\n",
        "\n",
        "print(f\"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ¸ˆã¿ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ '{OUTPUT_FILE}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUSnQkfL0xPi"
      },
      "source": [
        "# å½¢æ…‹ç´ è§£æ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzLghO_W5aNs"
      },
      "outputs": [],
      "source": [
        "from janome.tokenizer import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pLPB8_25flD",
        "outputId": "991596fe-4a8d-431f-e21b-f6ba281f95a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "åè©,ä»£åè©,ä¸€èˆ¬,*\n",
            "<class 'list'>\n",
            "åè©\n",
            "åŠ©è©,ä¿‚åŠ©è©,*,*\n",
            "<class 'list'>\n",
            "åŠ©è©\n",
            "å‹•è©,è‡ªç«‹,*,*\n",
            "<class 'list'>\n",
            "å‹•è©\n",
            "åŠ©è©,æ¥ç¶šåŠ©è©,*,*\n",
            "<class 'list'>\n",
            "åŠ©è©\n",
            "å‹•è©,éè‡ªç«‹,*,*\n",
            "<class 'list'>\n",
            "å‹•è©\n",
            "åè©,ä¸€èˆ¬,*,*\n",
            "<class 'list'>\n",
            "åè©\n",
            "åŠ©è©,æ ¼åŠ©è©,ä¸€èˆ¬,*\n",
            "<class 'list'>\n",
            "åŠ©è©\n",
            "å‹•è©,è‡ªç«‹,*,*\n",
            "<class 'list'>\n",
            "å‹•è©\n",
            "åŠ©å‹•è©,*,*,*\n",
            "<class 'list'>\n",
            "åŠ©å‹•è©\n",
            "åŠ©å‹•è©,*,*,*\n",
            "<class 'list'>\n",
            "åŠ©å‹•è©\n"
          ]
        }
      ],
      "source": [
        "t = Tokenizer()\n",
        "parsed_words = []\n",
        "a = \"ã‚ãŸã—ã¯èµ°ã£ã¦ã„ã‚‹çŠ¬ã‚’è¦‹ã¾ã—ãŸ\"\n",
        "TARGET_POS = ('åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©')\n",
        "\n",
        "for token in t.tokenize(a):\n",
        "    print(token.part_of_speech)\n",
        "    print(type(token.part_of_speech.split(\",\")))\n",
        "    pos = token.part_of_speech.split(',')[0]\n",
        "    print(pos)\n",
        "\n",
        "    if pos in TARGET_POS:\n",
        "        word = token.base_form\n",
        "\n",
        "        if word and word != '*':#word != \"*\"ã¯ã€åŸå½¢ã‚’ç‰¹å®šã§ããªã‹ã£ãŸæ™‚ã§ã‚‚æ­£ã—ãå‡¦ç†ã§ãã‚‹ã‚ˆã†ã«\n",
        "            parsed_words.append(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMM3POk_zw3T",
        "outputId": "99a77a0f-063a-4752-a8cc-fd4245f57269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿å®Œäº†ã€‚ç·æ–‡å­—æ•°: 2479119\n",
            "å½¢æ…‹ç´ è§£æã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†ã€‚ç·å˜èªæ•°: 808219\n",
            "æœ€çµ‚ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ 'soseki_corpus_tokenized.txt' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\n",
            "æ¬¡ã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Word2Vecãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n"
          ]
        }
      ],
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "import os\n",
        "\n",
        "INPUT_FILE = 'soseki_corpus_preprocessed.txt' # å‰å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«\n",
        "OUTPUT_FILE = 'soseki_corpus_tokenized.txt' # Word2Vecå­¦ç¿’ç”¨ã‚³ãƒ¼ãƒ‘ã‚¹ã®ä¿å­˜å…ˆ\n",
        "\n",
        "TARGET_POS = ('åè©', 'å‹•è©', 'å½¢å®¹è©', 'å‰¯è©')\n",
        "\n",
        "try:\n",
        "    t = Tokenizer()\n",
        "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        cleaned_text = f.read()\n",
        "\n",
        "    print(f\"ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿å®Œäº†ã€‚ç·æ–‡å­—æ•°: {len(cleaned_text)}\")\n",
        "\n",
        "    parsed_words = []\n",
        "\n",
        "    for token in t.tokenize(cleaned_text):#t.tokenizeã§ã€å½¢æ…‹ç´ ã«åˆ†è§£\n",
        "        pos = token.part_of_speech.split(',')[0]\n",
        "\n",
        "        if pos in TARGET_POS:\n",
        "            word = token.base_form\n",
        "\n",
        "            if word and word != '*':\n",
        "                parsed_words.append(word)\n",
        "\n",
        "    print(f\"å½¢æ…‹ç´ è§£æã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº†ã€‚ç·å˜èªæ•°: {len(parsed_words)}\")\n",
        "\n",
        "    # 4. æœ€çµ‚ã‚³ãƒ¼ãƒ‘ã‚¹ã®æ›¸ãå‡ºã—\n",
        "    # Word2VecãŒèª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ã€å˜èªã‚’åŠè§’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§çµåˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜\n",
        "    final_corpus = ' '.join(parsed_words)\n",
        "\n",
        "    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "        f.write(final_corpus)\n",
        "\n",
        "    print(f\"æœ€çµ‚ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ '{OUTPUT_FILE}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    print(f\"æ¬¡ã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Word2Vecãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ğŸš¨ ã‚¨ãƒ©ãƒ¼: '{INPUT_FILE}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
